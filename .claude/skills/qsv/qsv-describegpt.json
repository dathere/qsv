{
  "name": "qsv-describegpt",
  "version": "14.0.0",
  "description": "Infer a \"neuro-procedural\" (not quite \"neuro-symbolic\") Data Dictionary, Description & Tags or ask questions about a CSV with a configurable, Mini Jinja prompt file, using any OpenAI API-compatible LLM, including local LLMs like Ollama, Jan & LM Studio. (e.g. Markdown, JSON, TOON, Everything, Spanish, Mandarin, Controlled Tags; --prompt \"What are the top 10 complaint types by community board & borough by year?\" - deterministic, hallucination-free SQL RAG result; iterative, session-based SQL RAG refinement - refined SQL RAG result)",
  "category": "analysis",
  "command": {
    "binary": "qsv",
    "subcommand": "describegpt",
    "args": [
      {
        "name": "input",
        "type": "file",
        "required": true,
        "description": ""
      }
    ],
    "options": [
      {
        "flag": "--addl-cols",
        "type": "flag",
        "description": "Add additional columns to the dictionary from the Summary Statistics."
      },
      {
        "flag": "--addl-cols-list",
        "type": "string",
        "description": "A comma-separated list of additional stats columns to add to the dictionary. The columns must be present in the Summary Statistics. If the columns are not present in the Summary Statistics or already in the dictionary, they will be ignored. CONVENIENCE VALUES: These values are case-insensitive and automatically set the --addl-cols option to true. \"everything\" can be used to add all 45 \"available\" statistics columns. You can adjust the available columns with --stats-options. \"everything!\" automatically sets --stats-options to compute \"all\" 51 supported stats. The 6 addl cols are the mode/s & antimode/s stats with each having counts & occurrences. \"moar\" gets you even moar stats, with detailed outliers info. \"moar!\" gets you even moar with --advanced stats (Kurtosis, Gini Coefficient & Shannon Entropy)",
        "default": "sort_order, sortiness, mean, median, mad, stddev, variance, cv"
      },
      {
        "flag": "--addl-props",
        "type": "string",
        "description": "Additional model properties to pass to the LLM chat/completion API. Various models support different properties beyond the standard ones. For instance, gpt-oss-20b supports the \"reasoning_effort\" property. e.g. to set the \"reasoning_effort\" property to \"high\" & \"temperature\" to 0.5, use '{\"reasoning_effort\": \"high\", \"temperature\": 0.5}'"
      },
      {
        "flag": "--all",
        "type": "flag",
        "description": "Shortcut for --dictionary --description --tags."
      },
      {
        "flag": "--api-key",
        "type": "string",
        "description": "The API key to use. If set, takes precedence over the QSV_LLM_APIKEY envvar. Required when the base URL is not localhost. Set to NONE to suppress sending the API key."
      },
      {
        "flag": "--base-url",
        "type": "string",
        "description": "The LLM API URL. Supports APIs & local LLMs compatible with the OpenAI API specification. Some common base URLs: OpenAI: https://api.openai.com/v1 Gemini: https://generativelanguage.googleapis.com/v1beta/openai TogetherAI: https://api.together.ai/v1 Local LLMs: Ollama: http://localhost:11434/v1 Jan: https://localhost:1337/v1 LM Studio: http://localhost:1234/v1 NOTE: If set, takes precedence over the QSV_LLM_BASE_URL environment variable and the base URL specified in the prompt file.",
        "default": "http://localhost:1234/v1"
      },
      {
        "flag": "--cache-dir",
        "type": "string",
        "description": "The directory to use for caching downloaded tag vocabulary resources. If the directory does not exist, qsv will attempt to create it. If the QSV_CACHE_DIR envvar is set, it will be used instead.",
        "default": "~/.qsv-cache"
      },
      {
        "flag": "--ckan-api",
        "type": "string",
        "description": "The URL of the CKAN API to use for downloading tag vocabulary resources with the \"ckan://\" scheme. If the QSV_CKAN_API envvar is set, it will be used instead.",
        "default": "https://data.dathere.com/api/3/action"
      },
      {
        "flag": "--ckan-token",
        "type": "string",
        "description": "The CKAN API token to use. Only required if downloading private resources. If the QSV_CKAN_TOKEN envvar is set, it will be used instead."
      },
      {
        "flag": "--description",
        "type": "flag",
        "description": "Infer a general Description of the dataset based on detailed statistical context. An Attribution signature is embedded in the Description."
      },
      {
        "flag": "--dictionary",
        "type": "flag",
        "description": "Create a Data Dictionary using a hybrid \"neuro-procedural\" pipeline - i.e. the Dictionary is populated deterministically using Summary Statistics and Frequency Distribution data, and only the human-friendly Label and Description are populated by the LLM using the same statistical context."
      },
      {
        "flag": "--disk-cache-dir",
        "type": "string",
        "description": "The directory <dir> to store the disk cache. Note that if the directory does not exist, it will be created. If the directory exists, it will be used as is, and will not be flushed. This option allows you to maintain several disk caches for different describegpt jobs (e.g. one for a data portal, another for internal data exchange, etc.)",
        "default": "~/.qsv/cache/describegpt"
      },
      {
        "flag": "--enum-threshold",
        "type": "string",
        "description": "The threshold for compiling Enumerations with the frequency command before bucketing other unique values into the \"Other\" category. This is a convenience shortcut for --freq-options --limit <n>. If --freq-options contains --limit, this flag is ignored.",
        "default": "10"
      },
      {
        "flag": "--export-prompt",
        "type": "string",
        "description": "Export the default prompts to the specified file that can be used with the --prompt-file option. The file will be saved with a .toml extension. If the file already exists, it will be overwritten. It will exit after exporting the prompts."
      },
      {
        "flag": "--fewshot-examples",
        "type": "flag",
        "description": "By default, few-shot examples are NOT included in the LLM prompt when generating SQL queries. When this option is set, few-shot examples in the default prompt file are included. Though this will increase the quality of the generated SQL, it comes at a cost - increased LLM API call cost in terms of tokens and execution time. See https://en.wikipedia.org/wiki/Prompt_engineering for more info."
      },
      {
        "flag": "--flush-cache",
        "type": "flag",
        "description": "Flush the current cache entries on startup. WARNING: This operation is irreversible."
      },
      {
        "flag": "--forget",
        "type": "flag",
        "description": "Remove a cached response if it exists and then exit."
      },
      {
        "flag": "--format",
        "type": "string",
        "description": "Output format: Markdown, TSV, JSON, or TOON. TOON is a compact, human-readable encoding of the JSON data model for LLM prompts. See https://toonformat.dev/ for more info.",
        "default": "Markdown"
      },
      {
        "flag": "--freq-options",
        "type": "string",
        "description": "Options for the frequency command used to generate frequency distributions. You can use this to exclude certain variable types from frequency analysis (e.g., --select '!id,!uuid'), limit results differently per use case, or control output format. If --limit is specified here, it takes precedence over --enum-threshold. If it starts with \"file:\" prefix, the frequency data is read from the specified CSV file instead of running the frequency command. e.g. \"file:my_custom_frequency.csv\"",
        "default": "--rank-strategy dense"
      },
      {
        "flag": "--fresh",
        "type": "flag",
        "description": "Send a fresh request to the LLM API, refreshing a cached response if it exists. When a --prompt SQL query fails, you can also use this option to request the LLM to generate a new SQL query."
      },
      {
        "flag": "--language",
        "type": "string",
        "description": "The output language/dialect to use for the response. (e.g., \"Spanish\", \"French\", \"Hindi\", \"Mandarin\", \"Italian\", \"Castilian\", \"Franglais\", \"Taglish\", \"Pig Latin\", \"Valley Girl\", \"Pirate\", \"Shakespearean English\", \"Chavacano\", \"Gen Z\", \"Yoda\", etc.)"
      },
      {
        "flag": "--max-tokens",
        "type": "string",
        "description": "Limits the number of generated tokens in the output. Set to 0 to disable token limits. If the --base-url is localhost, indicating a local LLM, the default is automatically set to 0.",
        "default": "10000"
      },
      {
        "flag": "--model",
        "type": "string",
        "description": "The model to use for inferencing. If set, takes precedence over the QSV_LLM_MODEL environment variable.",
        "default": "openai/gpt-oss-20b"
      },
      {
        "flag": "--no-cache",
        "type": "flag",
        "description": "Disable default disk cache."
      },
      {
        "flag": "--num-examples",
        "type": "string",
        "description": "The number of Example values to include in the dictionary.",
        "default": "5"
      },
      {
        "flag": "--num-tags",
        "type": "string",
        "description": "The maximum number of tags to infer when the --tags option is used. Maximum allowed value is 50.",
        "default": "10"
      },
      {
        "flag": "--output",
        "type": "string",
        "description": "Write output to <file> instead of stdout. If --format is set to TSV, separate files will be created for each prompt type with the pattern {filestem}.{kind}.tsv (e.g., output.dictionary.tsv, output.tags.tsv)."
      },
      {
        "flag": "--prompt",
        "type": "string",
        "description": "Custom prompt to answer questions about the dataset. The prompt will be answered based on the dataset's Summary Statistics, Frequency data & Data Dictionary. If the prompt CANNOT be answered by looking at these metadata, a SQL query will be generated to answer the question. If the \"polars\" or the \"QSV_DESCRIBEGPT_DB_ENGINE\" environment variable is set & the `--sql-results` option is used, the SQL query will be automatically executed and its results returned. Otherwise, the SQL query will be returned along with the reasoning behind it. If it starts with \"file:\" prefix, the prompt is read from the file specified. e.g. \"file:my_long_prompt.txt\""
      },
      {
        "flag": "--prompt-file",
        "type": "string",
        "description": "The configurable TOML file containing prompts to use for inferencing. If no file is provided, default prompts will be used. The prompt file uses the Mini Jinja template engine (https://docs.rs/minijinja) See https://github.com/dathere/qsv/blob/master/resources/describegpt_defaults.toml"
      },
      {
        "flag": "--sample-size",
        "type": "string",
        "description": "The number of rows to randomly sample from the input file for the sample data. Uses the INDEXED sampling method with the qsv sample command.",
        "default": "100"
      },
      {
        "flag": "--session",
        "type": "string",
        "description": "Enable stateful session mode for iterative SQL RAG refinement. The session name is the file path of the markdown file where session messages will be stored. When used with --prompt, subsequent queries in the same session will refine the baseline SQL query. SQL query results (10-row sample) and errors are automatically included in subsequent messages for context."
      },
      {
        "flag": "--session-len",
        "type": "string",
        "description": "Maximum number of recent messages to keep in session context before summarizing older messages. Only used when --session is specified.",
        "default": "10"
      },
      {
        "flag": "--sql-results",
        "type": "string",
        "description": "The file to save the SQL query results to. Only valid if the --prompt option is used & the \"polars\" or the \"QSV_DESCRIBEGPT_DB_ENGINE\" environment variable is set. If the SQL query executes successfully, the results will be saved with a \".csv\" extension. Otherwise, it will be saved with a \".sql\" extension so the user can inspect why it failed and modify it."
      },
      {
        "flag": "--stats-options",
        "type": "string",
        "description": "Options for the stats command used to generate summary statistics. If it starts with \"file:\" prefix, the statistics are read from the specified CSV file instead of running the stats command. e.g. \"file:my_custom_stats.csv\"",
        "default": "--infer-dates --infer-boolean --mad --quartiles --percentiles --force --stats-jsonl"
      },
      {
        "flag": "--tag-vocab",
        "type": "string",
        "description": "The CSV file containing the tag vocabulary to use for inferring tags. If no tag vocabulary file is provided, the model will use free-form tags. Supports local files, remote URLs (http/https), CKAN resources (ckan://), and dathere:// scheme. Remote resources are cached locally. The CSV file must have two columns with headers: first column is the tag, second column is the description. Note that qsvlite only supports local files."
      },
      {
        "flag": "--tags",
        "type": "flag",
        "description": "Infer Tags that categorize the dataset based on detailed statistical context. Useful for grouping datasets and filtering."
      },
      {
        "flag": "--timeout",
        "type": "string",
        "description": "Timeout for completions in seconds. If 0, no timeout is used.",
        "default": "300"
      },
      {
        "flag": "--truncate-str",
        "type": "string",
        "description": "The maximum length of an Example value in the dictionary. An ellipsis is appended to the truncated value. If zero, no truncation is performed.",
        "default": "25"
      }
    ]
  },
  "hints": {
    "streamable": true,
    "memory": "constant"
  }
}