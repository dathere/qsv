{
  "name": "qsv-split",
  "version": "12.0.0",
  "description": "Splits the given CSV data into chunks. It has three modes: by size (rowcount), by number of chunks and by kb-size. See `partition` command for splitting by a column value. When splitting by size, the CSV data is split into chunks of the given number of rows. The last chunk may have fewer rows if the number of records is not evenly divisible by the given rowcount. When splitting by number of chunks, the CSV data is split into the given number of chunks. The number of rows in each chunk is determined by the number of records in the CSV data and the number of desired chunks. If the number of records is not evenly divisible by the number of chunks, the last chunk will have fewer records. When splitting by kb-size, the CSV data is split into chunks of the given size in kilobytes. The number of rows in each chunk may vary, but the size of each chunk will not exceed the desired size. Uses multithreading to go faster if the CSV has an index when splitting by size or by number of chunks. Splitting by kb-size is always done sequentially with a single thread. The default is to split by size with a chunk size of 500. The files are written to the directory given with the name '{start}.csv', where {start} is the index of the first record of the chunk (starting at 0).",
  "category": "utility",
  "command": {
    "binary": "qsv",
    "subcommand": "split",
    "args": [
      {
        "name": "input",
        "type": "file",
        "required": true,
        "description": "The CSV file to read. If not given, input is read from STDIN."
      },
      {
        "name": "outdir",
        "type": "string",
        "required": true,
        "description": "The directory where the output files will be written. If it does not exist, it will be created."
      }
    ],
    "options": [
      {
        "flag": "--chunks",
        "type": "string",
        "description": "The number of chunks to split the data into. This option is mutually exclusive with --size. The number of rows in each chunk is determined by the number of records in the CSV data and the number of desired chunks. If the number of records is not evenly divisible by the number of chunks, the last chunk will have fewer records."
      },
      {
        "flag": "--delimiter",
        "type": "string",
        "description": "The field delimiter for reading CSV data. Must be a single character. (default: ,)"
      },
      {
        "flag": "--filename",
        "type": "string",
        "description": "A filename template to use when constructing the names of the output files.  The string '{}' will be replaced by the zero-based row number of the first row in the chunk. [default: {}.csv]",
        "default": "{}.csv"
      },
      {
        "flag": "--filter",
        "type": "string",
        "description": "Run the specified command on each chunk after it is written. The command should use the FILE environment variable ($FILE on Linux/macOS, %FILE% on Windows), which is set to the path of the output file for each chunk. The string '{}' in the command will be replaced by the zero-based row number of the first row in the chunk."
      },
      {
        "flag": "--filter-cleanup",
        "type": "flag",
        "description": "Cleanup the original output filename AFTER the filter command is run successfully for EACH chunk. If the filter command is not successful, the original filename is not removed. Only valid when --filter is used."
      },
      {
        "flag": "--filter-ignore-errors",
        "type": "flag",
        "description": "Ignore errors when running the filter command. Only valid when --filter is used."
      },
      {
        "flag": "--help",
        "type": "flag",
        "description": "Display this message"
      },
      {
        "flag": "--jobs",
        "type": "string",
        "description": "The number of splitting jobs to run in parallel. This only works when the given CSV data has an index already created. Note that a file handle is opened for each job. When not set, the number of jobs is set to the number of CPUs detected."
      },
      {
        "flag": "--kb-size",
        "type": "string",
        "description": "The size of each chunk in kilobytes. The number of rows in each chunk may vary, but the size of each chunk will not exceed the desired size. This option is mutually exclusive with --size and --chunks."
      },
      {
        "flag": "--no-headers",
        "type": "flag",
        "description": "When set, the first row will NOT be interpreted as column names. Otherwise, the first row will appear in all chunks as the header row."
      },
      {
        "flag": "--pad",
        "type": "string",
        "description": "The zero padding width that is used in the generated filename. [default: 0]",
        "default": "0"
      },
      {
        "flag": "--quiet",
        "type": "flag",
        "description": "Do not display an output summary to stderr."
      },
      {
        "flag": "--size",
        "type": "string",
        "description": "The number of records to write into each chunk. [default: 500]",
        "default": "500"
      }
    ]
  },
  "examples": [
    {
      "description": "outdir --size 100 --filename chunk_{}.csv input.csv",
      "command": "qsv split outdir --size 100 --filename chunk_{}.csv input.csv"
    },
    {
      "description": "in the directory 'outdir', creating the directory if it does not exist.",
      "command": "qsv split outdir/subdir -s 100 --filename chunk_{}.csv --pad 5 input.csv"
    },
    {
      "description": "in the directory 'outdir/subdir', creating the directories if they do not exist.",
      "command": "qsv split . -s 100 input.csv"
    },
    {
      "description": "This will create files like 0.csv, 100.csv, etc. in the current directory.",
      "command": "qsv split outdir --kb-size 1000 input.csv"
    },
    {
      "description": "to 1000KB in size.",
      "command": "cat in.csv | qsv split mysplitoutput -s 1000"
    },
    {
      "description": "outdir --chunks 10 input.csv",
      "command": "qsv split outdir --chunks 10 input.csv"
    },
    {
      "description": "splitoutdir -c 10 -j 4 input.csv",
      "command": "qsv split splitoutdir -c 10 -j 4 input.csv"
    },
    {
      "description": "outdir -s 100 --filter \"gzip $FILE\" input.csv",
      "command": "qsv split outdir -s 100 --filter \"gzip $FILE\" input.csv"
    },
    {
      "description": "'outdir', and then run the command \"gzip\" on each chunk.",
      "command": "qsv split outdir --filter \"powershell Compress-Archive -Path $FILE -Destination {}.zip\" input.csv"
    }
  ],
  "hints": {
    "streamable": true,
    "memory": "constant"
  },
  "test_file": "https://github.com/dathere/qsv/blob/master/tests/test_split.rs"
}