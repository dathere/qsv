# qsv Command Help

> Auto-generated from qsv command USAGE text. See [README](../../README.md) for full documentation.

| Command | Description |
| --- | --- |
| [apply](apply.md)<br>ğŸ“‡ğŸš€ğŸ§ ğŸ¤–ğŸ”£ğŸ‘† | Apply series of string, date, math & currency transformations to given CSV column/s. It also has some basic [NLP](https://en.wikipedia.org/wiki/Natural_language_processing) functions ([similarity](https://crates.io/crates/strsim), [sentiment analysis](https://crates.io/crates/vader_sentiment), [profanity](https://docs.rs/censor/latest/censor/), [eudex](https://github.com/ticki/eudex#eudex-a-blazingly-fast-phonetic-reductionhashing-algorithm), [language](https://crates.io/crates/whatlang) & [name gender](https://github.com/Raduc4/gender_guesser?tab=readme-ov-file#gender-guesser)) detection. |
| [applydp](applydp.md)<br>ğŸ“‡ğŸš€ğŸ”£ğŸ‘† ![CKAN](../images/ckan.png) | applydp is a slimmed-down version of `apply` with only [Datapusher+](https://github.com/dathere/datapusher-plus) relevant subcommands/operations (`qsvdp` binary variant only). |
| [behead](behead.md) | Drop headers from a CSV. |
| [cat](cat.md)<br>ğŸ—„ï¸ | Concatenate CSV files by row or by column. |
| [clipboard](clipboard.md)<br>ğŸ–¥ï¸ | Provide input from the clipboard or save output to the clipboard. |
| [color](color.md)<br>ğŸ»â€â„ï¸ğŸ–¥ï¸ | Outputs tabular data as a pretty, colorized table that always fits into the terminal. Apart from CSV and its dialects, Arrow, Avro/IPC, Parquet, JSON array & JSONL formats are supported with the "polars" feature. |
| [count](count.md)<br>ğŸ“‡ğŸï¸ğŸ»â€â„ï¸ | Count the rows and optionally compile record width statistics of a CSV file. (11.87 seconds for a 15gb, 27m row NYC 311 dataset without an index. Instantaneous with an index.) If the `polars` feature is enabled, uses Polars' multithreaded, mem-mapped CSV reader for fast counts even without an index |
| [datefmt](datefmt.md)<br>ğŸ“‡ğŸš€ğŸ‘† | Formats recognized date fields ([19 formats recognized](https://docs.rs/qsv-dateparser/latest/qsv_dateparser/#accepted-date-formats)) to a specified date format using [strftime date format specifiers](https://docs.rs/chrono/latest/chrono/format/strftime/). |
| [dedup](dedup.md)<br>ğŸ¤¯ğŸš€ğŸ‘† | Remove duplicate rows (See also `extdedup`, `extsort`, `sort` & `sortcheck` commands). |
| [describegpt](describegpt.md)<br>ğŸŒğŸ¤–ğŸª„ğŸ—ƒï¸ğŸ“šâ›©ï¸ ![CKAN](../images/ckan.png) | Infer a ["neuro-symbolic"](https://en.wikipedia.org/wiki/Neuro-symbolic_AI) Data Dictionary, Description & Tags or ask questions about a CSV with a [configurable, Mini Jinja prompt file](../../resources/describegpt_defaults.toml), using any [OpenAI API](https://platform.openai.com/docs/introduction)-compatible LLM, including local LLMs via [Ollama](https://ollama.com), [Jan](https://jan.ai) & [LM Studio](https://lmstudio.ai/). (e.g. [Markdown](../describegpt/nyc311-describegpt.md), [JSON](../describegpt/nyc311-describegpt.json), [TOON](../describegpt/nyc311-describegpt.toon), [Everything](../describegpt/nyc311-describegpt-everything.md), [Spanish](../describegpt/nyc311-describegpt-spanish.md), [Mandarin](../describegpt/nyc311-describegpt-mandarin.md), [Controlled Tags](../describegpt/nyc311-describegpt-tagvocab.md); [--prompt "What are the top 10 complaint types by community board & borough by year?"](../describegpt/nyc311-describegpt-prompt.md) - [deterministic, hallucination-free SQL RAG result](../describegpt/nyc311-describegpt-prompt.csv); [iterative, session-based SQL RAG refinement](../describegpt/allegheny_discussion3.md) - [refined SQL RAG result](../describegpt/mostexpensive6.csv)) |
| [diff](diff.md)<br>ğŸš€ğŸª„ | Find the difference between two CSVs with ludicrous speed! e.g. _compare two CSVs with 1M rows x 9 columns in under 600ms!_ |
| [edit](edit.md) | Replace the value of a cell specified by its row and column. |
| [enum](enum.md)<br>ğŸ‘† | Add a new column enumerating rows by adding a column of incremental or uuid identifiers. Can also be used to copy a column or fill a new column with a constant value. |
| [excel](excel.md)<br>ğŸš€ | Exports a specified Excel/ODS sheet to a CSV file. |
| [exclude](exclude.md)<br>ğŸ“‡ğŸ‘† | Removes a set of CSV data from another set based on the specified columns. |
| [explode](explode.md)<br>ğŸ”£ğŸ‘† | Explode rows into multiple ones by splitting a column value based on the given separator. |
| [extdedup](extdedup.md)<br>ğŸ‘† | Remove duplicate rows from an arbitrarily large CSV/text file using a memory-mapped, [on-disk hash table](https://crates.io/crates/odht). Unlike the `dedup` command, this command does not load the entire file into memory nor does it sort the deduped file. |
| [extsort](extsort.md)<br>ğŸš€ğŸ“‡ğŸ‘† | Sort an arbitrarily large CSV/text file using a multithreaded [external merge sort](https://en.wikipedia.org/wiki/External_sorting) algorithm. |
| [fetch](fetch.md)<br>ğŸ“‡ğŸ§ ğŸŒ | Send/Fetch data to/from web services for every row using **HTTP Get**. Comes with [HTTP/2](https://http2-explained.haxx.se/en/part1) [adaptive flow control](https://medium.com/coderscorner/http-2-flow-control-77e54f7fd518), [jaq](https://github.com/01mf02/jaq?tab=readme-ov-file#jaq) JSON query language support, dynamic throttling ([RateLimit](https://www.ietf.org/archive/id/draft-ietf-httpapi-ratelimit-headers-06.html)) & caching with available persistent caching using [Redis](https://redis.io/) or a disk-cache. |
| [fetchpost](fetchpost.md)<br>ğŸ“‡ğŸ§ ğŸŒâ›©ï¸ | Similar to `fetch`, but uses **HTTP Post** ([HTTP GET vs POST methods](https://www.geeksforgeeks.org/difference-between-http-get-and-post-methods/)). Supports HTML form (application/x-www-form-urlencoded), JSON (application/json) and custom content types - with the ability to render payloads using CSV data using the [Mini Jinja](https://docs.rs/minijinja/latest/minijinja/) template engine. |
| [fill](fill.md)<br>ğŸ‘† | Fill empty values. |
| [fixlengths](fixlengths.md) | Force a CSV to have same-length records by either padding or truncating them. |
| [flatten](flatten.md) | A flattened view of CSV records. Useful for viewing one record at a time. e.g. `qsv slice -i 5 data.csv \| qsv flatten`. |
| [fmt](fmt.md) | Reformat a CSV with different delimiters, record terminators or quoting rules. (Supports ASCII delimited data.) |
| [foreach](foreach.md)<br>ğŸ“‡ | Execute a shell command once per record in a given CSV file. |
| [frequency](frequency.md)<br>ğŸ“‡ğŸ˜£ğŸï¸ğŸ‘†ğŸª„![Luau](../images/luau.png) | Build [frequency distribution tables](https://en.wikipedia.org/wiki/Frequency_(statistics)) of each column. Uses multithreading to go faster if an index is present (Examples: [CSV](../../scripts/nyc311-1m.freqs.csv) [JSON](../../scripts/nyc311-1m.freqs.json) [TOON](../../scripts/nyc311-1m.freqs.toon)). |
| [geocode](geocode.md)<br>ğŸ“‡ğŸ§ ğŸŒğŸš€ğŸ”£ğŸ‘†ğŸŒ | Geocodes a location against an updatable local copy of the [Geonames](https://www.geonames.org/) cities & the [Maxmind GeoLite2](https://www.maxmind.com/en/geolite-free-ip-geolocation-data) databases. With caching and multi-threading, it geocodes up to 360,000 records/sec! |
| [geoconvert](geoconvert.md)<br>ğŸŒ | Convert between various spatial formats and CSV/SVG including GeoJSON, SHP, and more. |
| [headers](headers.md)<br>ğŸ—„ï¸ | Show the headers of a CSV. Or show the intersection of all headers between many CSV files. |
| [index](index.md) | Create an index for a CSV. This is very quick (even the 15gb, 28m row NYC 311 dataset takes all of 14 seconds to index) & provides constant time indexing/random access into the CSV. With an index, `count`, `sample` & `slice` work instantaneously; random access mode is enabled in `luau`; and multithreading is enabled for the `frequency`, `split`, `stats`, `schema` & `tojsonl` commands. |
| [input](input.md) | Read CSV data with special commenting, quoting, trimming, line-skipping & non-UTF8 encoding handling rules. Typically used to "normalize" a CSV for further processing with other qsv commands. |
| [join](join.md)<br>ğŸ˜£ğŸ‘† | Inner, outer, right, cross, anti & semi joins. Automatically creates a simple, in-memory hash index to make it fast. |
| [joinp](joinp.md)<br>ğŸš€ğŸ»â€â„ï¸ğŸª„ | Inner, outer, right, cross, anti, semi, non-equi & asof joins using the [Pola.rs](https://www.pola.rs) engine. Unlike the `join` command, `joinp` can process files larger than RAM, is multithreaded, has join key validation, a maintain row order option, pre and post-join filtering, join keys unicode normalization, supports "special" [non-equi joins](https://docs.pola.rs/user-guide/transformations/joins/#non-equi-joins) and [asof joins](https://docs.pola.rs/user-guide/transformations/joins/#asof-join) (which is [particularly useful for time series data](https://github.com/dathere/qsv/blob/30cc920d0812a854fcbfedc5db81788a0600c92b/tests/test_joinp.rs#L509-L983)) & its output columns can be coalesced. |
| [json](json.md)<br>ğŸ‘† | Convert JSON array to CSV. |
| [jsonl](jsonl.md)<br>ğŸš€ğŸ”£ | Convert newline-delimited JSON ([JSONL](https://jsonlines.org/)/[NDJSON](http://ndjson.org/)) to CSV. See `tojsonl` command to convert CSV to JSONL. |
| [lens](lens.md)<br>ğŸ»â€â„ï¸ğŸ–¥ï¸ | Interactively view, search & filter tabular data files using the [csvlens](https://github.com/YS-L/csvlens#csvlens) engine. Apart from CSV and its dialects, Arrow, Avro/IPC, Parquet, JSON array & JSONL formats are supported with the "polars" feature. |
| [luau](luau.md)<br>ğŸ“‡ğŸŒğŸ”£ğŸ“š ![CKAN](../images/ckan.png) | Create multiple new computed columns, filter rows, compute aggregations and build complex data pipelines by executing a [Luau](https://luau-lang.org) [0.708](https://github.com/Roblox/luau/releases/tag/0.708) expression/script for every row of a CSV file ([sequential mode](https://github.com/dathere/qsv/blob/bb72c4ef369d192d85d8b7cc6e972c1b7df77635/tests/test_luau.rs#L254-L298)), or using [random access](https://www.webopedia.com/definitions/random-access/) with an index ([random access mode](https://github.com/dathere/qsv/blob/bb72c4ef369d192d85d8b7cc6e972c1b7df77635/tests/test_luau.rs#L367-L415)). Can process a single Luau expression or [full-fledged data-wrangling scripts using lookup tables](https://github.com/dathere/qsv-lookup-tables#example) with discrete BEGIN, MAIN and END sections. It is not just another qsv command, it is qsv's [Domain-specific Language](https://en.wikipedia.org/wiki/Domain-specific_language) (DSL) with [numerous qsv-specific helper functions](https://github.com/dathere/qsv/blob/113eee17b97882dc368b2e65fec52b86df09f78b/src/cmd/luau.rs#L1356-L2290) to build production data pipelines. |
| [moarstats](moarstats.md)<br>ğŸ“‡ğŸï¸ | Add dozens of additional statistics, including extended outlier, robust & bivariate statistics to an existing stats CSV file. ([example](../moarstats/NYC_311_SR_2010-2020-sample-1M.stats.csv)). |
| [partition](partition.md)<br>ğŸ‘† | Partition a CSV based on a column value. |
| [pivotp](pivotp.md)<br>ğŸš€ğŸ»â€â„ï¸ğŸª„ | Pivot CSV data. Features "smart" aggregation auto-selection based on data type & stats. |
| [pragmastat](pragmastat.md)<br>ğŸ¤¯ | Compute pragmatic statistics using the [Pragmastat](https://pragmastat.dev/) library. |
| [pro](pro.md) | Interact with the [qsv pro](https://qsvpro.dathere.com) API. |
| [prompt](prompt.md)<br>ğŸ»â€â„ï¸ğŸ–¥ï¸ | Open a file dialog to either pick a file as input or save output to a file. |
| [pseudo](pseudo.md)<br>ğŸ”£ğŸ‘† | [Pseudonymise](https://en.wikipedia.org/wiki/Pseudonymization) the value of the given column by replacing them with an incremental identifier. |
| [py](py.md)<br>ğŸ“‡ğŸ”£ | Create a new computed column or filter rows by evaluating a Python expression on every row of a CSV file. Python's [f-strings](https://www.freecodecamp.org/news/python-f-strings-tutorial-how-to-use-f-strings-for-string-formatting/) is particularly useful for extended formatting, [with the ability to evaluate Python expressions as well](https://github.com/dathere/qsv/blob/4cd00dca88addf0d287247fa27d40563b6d46985/src/cmd/python.rs#L23-L31). [Requires Python 3.8 or greater](https://github.com/dathere/qsv/blob/master/docs/INTERPRETERS.md#building-qsv-with-python-feature). |
| [rename](rename.md) | Rename the columns of a CSV efficiently. |
| [replace](replace.md)<br>ğŸ“‡ğŸ‘†ğŸï¸ | Replace CSV data using a regex. Applies the regex to each field individually. |
| [reverse](reverse.md)<br>ğŸ“‡ğŸ¤¯ | Reverse order of rows in a CSV. Unlike the `sort --reverse` command, it preserves the order of rows with the same key. If an index is present, it works with constant memory. Otherwise, it will load all the data into memory. |
| [safenames](safenames.md)<br>![CKAN](../images/ckan.png) | Modify headers of a CSV to only have ["safe" names](../..//src/cmd/safenames.rs#L5-L14) - guaranteed "database-ready"/"CKAN-ready" names. |
| [sample](sample.md)<br>ğŸ“‡ğŸŒğŸï¸ | Randomly draw rows (with optional seed) from a CSV using seven different sampling methods - [reservoir](https://en.wikipedia.org/wiki/Reservoir_sampling) (default), [indexed](https://en.wikipedia.org/wiki/Random_access), [bernoulli](https://en.wikipedia.org/wiki/Bernoulli_sampling), [systematic](https://en.wikipedia.org/wiki/Systematic_sampling), [stratified](https://en.wikipedia.org/wiki/Stratified_sampling), [weighted](https://doi.org/10.1016/j.ipl.2005.11.003) & [cluster sampling](https://en.wikipedia.org/wiki/Cluster_sampling). Supports sampling from CSVs on remote URLs. |
| [schema](schema.md)<br>ğŸ“‡ğŸ˜£ğŸï¸ğŸ‘†ğŸª„ğŸ»â€â„ï¸ | Infer either a [JSON Schema Validation Draft 2020-12](https://json-schema.org/draft/2020-12/json-schema-validation) ([Example](https://github.com/dathere/qsv/blob/master/resources/test/311_Service_Requests_from_2010_to_Present-2022-03-04.csv.schema.json)) or [Polars Schema](https://docs.pola.rs/user-guide/lazy/schemas/) ([Example](https://github.com/dathere/qsv/blob/master/resources/test/NYC_311_SR_2010-2020-sample-1M.pschema.json)) from CSV data. In JSON Schema Validation mode, it produces a `.schema.json` file replete with inferred data type & domain/range validation rules derived from [`stats`](#stats_deeplink). Uses multithreading to go faster if an index is present. See [`validate`](#validate_deeplink) command to use the generated JSON Schema to validate if similar CSVs comply with the schema. With the `--polars` option, it produces a `.pschema.json` file that all polars commands (`sqlp`, `joinp` & `pivotp`) use to determine the data type of each column & to optimize performance. Both schemas are editable and can be fine-tuned. For JSON Schema, to refine the inferred validation rules. For Polars Schema, to change the inferred Polars data types. |
| [search](search.md)<br>ğŸ“‡ğŸï¸ğŸ‘† | Run a regex over a CSV. Applies the regex to selected fields & shows only matching rows. |
| [searchset](searchset.md)<br>ğŸ“‡ğŸï¸ğŸ‘† | _Run multiple regexes over a CSV in a single pass._ Applies the regexes to each field individually & shows only matching rows. |
| [select](select.md)<br>ğŸ‘† | Select, re-order, reverse, duplicate or drop columns. |
| [slice](slice.md)<br>ğŸ“‡ğŸï¸ğŸ—ƒï¸ | Slice rows from any part of a CSV. When an index is present, this only has to parse the rows in the slice (instead of all rows leading up to the start of the slice). |
| [snappy](snappy.md)<br>ğŸš€ğŸŒ | Does streaming compression/decompression of the input using Google's [Snappy](https://github.com/google/snappy/blob/main/docs/README.md) framing format ([more info](#automatic-compressiondecompression)). |
| [sniff](sniff.md)<br>ğŸ“‡ğŸŒğŸ¤– ![CKAN](../images/ckan.png) | Quickly sniff & infer CSV metadata (delimiter, header row, preamble rows, quote character, flexible, is_utf8, average record length, number of records, content length & estimated number of records if sniffing a CSV on a URL, number of fields, field names & data types). It is also a general mime type detector. |
| [sort](sort.md)<br>ğŸš€ğŸ¤¯ğŸ‘† | Sorts CSV data in [lexicographical](https://en.wikipedia.org/wiki/Lexicographic_order), [natural](https://en.wikipedia.org/wiki/Natural_sort_order), numerical, reverse, unique or random (with optional seed) order (Also see `extsort` & `sortcheck` commands). |
| [sortcheck](sortcheck.md)<br>ğŸ“‡ğŸ‘† | Check if a CSV is sorted. With the --json options, also retrieve record count, sort breaks & duplicate count. |
| [split](split.md)<br>ğŸ“‡ğŸï¸ | Split one CSV file into many CSV files. It can split by number of rows, number of chunks or file size. Uses multithreading to go faster if an index is present when splitting by rows or chunks. |
| [sqlp](sqlp.md)<br>ğŸ“‡ğŸš€ğŸ»â€â„ï¸ğŸ—„ï¸ğŸª„ | Run [Polars](https://pola.rs) SQL (a PostgreSQL dialect) queries against several CSVs, Parquet, JSONL and Arrow files - converting queries to blazing-fast Polars [LazyFrame](https://docs.pola.rs/user-guide/lazy/) expressions, processing larger than memory CSV files. Query results can be saved in CSV, JSON, JSONL, Parquet, Apache Arrow IPC and Apache Avro formats. |
| [stats](stats.md)<br>ğŸ“‡ğŸ¤¯ğŸï¸ğŸ‘†ğŸª„ | Compute [summary statistics](https://en.wikipedia.org/wiki/Summary_statistics) (sum, min/max/range, sort order/sortiness, min/max/sum/avg length, mean, standard error of the mean (SEM), geometric/harmonic means, stddev, variance, Coefficient of Variation (CV), nullcount, max precision, sparsity, quartiles, Interquartile Range (IQR), lower/upper fences, skewness, median, mode/s, antimode/s, cardinality & uniqueness ratio) & make GUARANTEED data type inferences (Null, String, Float, Integer, Date, DateTime, Boolean) for each column in a CSV ([Example](https://github.com/dathere/qsv/blob/master/scripts/NYC_311_SR_2010-2020-sample-1M.stats.csv) - [more info](https://github.com/dathere/qsv/wiki/Supplemental#stats-command-output-explanation)). Uses multithreading to go faster if an index is present (with an index, can compile "streaming" stats on NYC's 311 data (15gb, 28m rows) in less than 7.3 seconds!). |
| [table](table.md)<br>ğŸ¤¯ | Align output of a CSV using [elastic tabstops](https://github.com/BurntSushi/tabwriter) for viewing; or to create an "aligned TSV" file or Fixed Width Format file. To interactively view a CSV, use the `lens` command. |
| [template](template.md)<br>ğŸ“‡ğŸš€ğŸ”£ğŸ“šâ›©ï¸![CKAN](../images/ckan.png) | Renders a template using CSV data with the [Mini Jinja](https://docs.rs/minijinja/latest/minijinja/) template engine ([Example](https://github.com/dathere/qsv/blob/4645ec07b5befe3b0c0e49bf0f547315d0d7514b/src/cmd/template.rs#L18-L44)). |
| [to](to.md)<br>ğŸš€ğŸ—„ï¸ | Convert CSV files to [PostgreSQL](https://www.postgresql.org), [SQLite](https://www.sqlite.org/index.html), Excel (XLSX), [LibreOffice Calc](https://www.libreoffice.org/discover/calc/) (ODS) and [Data Package](https://datahub.io/docs/data-packages/tabular). |
| [tojsonl](tojsonl.md)<br>ğŸ“‡ğŸ˜£ğŸš€ğŸ”£ğŸª„ğŸ—ƒï¸ | Smartly converts CSV to a newline-delimited JSON ([JSONL](https://jsonlines.org/)/[NDJSON](http://ndjson.org/)). By scanning the CSV first, it "smartly" infers the appropriate JSON data type for each column. See `jsonl` command to convert JSONL to CSV. |
| [transpose](transpose.md)<br>ğŸ¤¯ğŸ‘† | Transpose rows/columns of a CSV. |
| [validate](validate.md)<br>ğŸ“‡ğŸš€ğŸŒğŸ“šğŸ—„ï¸![CKAN](../images/ckan.png) | Validate CSV data [_blazingly-fast_](https://github.com/Stranger6667/jsonschema-rs?tab=readme-ov-file#performance "using jsonschema-rs - the fastest JSON Schema validator for Rust") using [JSON Schema Validation (Draft 2020-12)](https://json-schema.org/draft/2020-12/json-schema-validation.html) (e.g. _up to 780,031 rows/second_[^1] using [NYC's 311 schema](https://github.com/dathere/qsv/blob/master/resources/test/311_Service_Requests_from_2010_to_Present-2022-03-04.csv.schema.json) generated by the [`schema`](#schema_deeplink) command) & put invalid records into a separate file along with a detailed validation error report. Supports several custom JSON Schema formats & keywords: * `currency` custom format with [ISO-4217](https://en.wikipedia.org/wiki/ISO_4217) validation * `dynamicEnum` custom keyword that supports enum validation against a CSV on the filesystem or a URL (http/https/ckan & dathere URL schemes supported) * `uniqueCombinedWith` custom keyword to validate uniqueness across multiple columns for composite key validation. If no JSON schema file is provided, validates if a CSV conforms to the [RFC 4180 standard](#rfc-4180-csv-standard) and is UTF-8 encoded. |

---

### Legend

âœ¨: enabled by a [feature flag](#feature-flags).  
ğŸ“‡: uses an index when available.  
ğŸ¤¯: loads entire CSV into memory, though `dedup`, `stats` & `transpose` have "streaming" modes as well.  
ğŸ˜£: uses additional memory proportional to the cardinality of the columns in the CSV.  
ğŸ§ : expensive operations are memoized with available inter-session Redis/Disk caching for fetch commands.  
ğŸ—„ï¸: [Extended input support](#extended-input-support).  
ğŸ—ƒï¸: [Limited Extended input support](#limited-extended-input-support).  
ğŸ»â€â„ï¸: command powered/accelerated by [![polars 0.53.0:ceb6366](https://img.shields.io/badge/polars-0.53.0:ceb6366-blue?logo=polars  
)](https://github.com/pola-rs/polars/releases/tag/rs-0.53.0) vectorized query engine.  
ğŸ¤–: command uses Natural Language Processing or Generative AI.  
ğŸï¸: multithreaded and/or faster when an index (ğŸ“‡) is available.  
ğŸš€: multithreaded even without an index.  
![CKAN](../images/ckan.png) : has [CKAN](https://ckan.org)-aware integration options.  
ğŸŒ: has web-aware options.  
ğŸ”£: requires UTF-8 encoded input.  
ğŸ‘†: has powerful column selector support. See [`select`](https://github.com/dathere/qsv/blob/master/src/cmd/select.rs#L2) for syntax.  
ğŸª„: "automagical" commands that uses stats and/or frequency tables to work "smarter" & "faster".  
ğŸ“š: has lookup table support, enabling runtime "lookups" against local or remote reference CSVs.  
ğŸŒ: has geospatial capabilities.  
â›©ï¸: uses [Mini Jinja](https://docs.rs/minijinja/latest/minijinja/) template engine.  
![Luau](../images/luau.png) : uses [Luau](https://luau.org/) [0.708](https://github.com/Roblox/luau/releases/tag/0.708) as an embedded scripting [DSL](https://en.wikipedia.org/wiki/Domain-specific_language).  
ğŸ–¥ï¸: part of the User Interface (UI) feature group  

---
**[README](../../README.md)**
