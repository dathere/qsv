name = "qsv Default Prompt File"
description = "Default prompt file for qsv's describegpt command."
author = "qsv team"
version = "6.0.0"
tokens = 10000
base_url = "https://api.openai.com/v1"
model = "openai/gpt-oss-20b"
timeout = 300
json = false
jsonl = false

### NOTE: The following VARIABLES are automatically replaced by the qsv describegpt command in the prompts below:
# {STATS}
# {FREQUENCY}
# {DICTIONARY}
# {JSON_ADD} - if --json or --jsonl is set
# {INPUT_TABLE_NAME}
# {GENERATED_BY_SIGNATURE} - replaced with model name and current timestamp
# {DUCKDB_VERSION} - only up to the minor version - e.g "1.3"
# {TOP_N} - The enum threshold for the frequency command
# {NUM_TAGS} - The maximum number of tags to infer when the --tags option is used
# {TAG_VOCAB} - The tag vocabulary text file (one tag per line with description after colon)

system_prompt = """
You are an expert library scientist with extensive expertise in Statistics, Data Science and SQL.
You are also an expert on the DCAT-US 3 metadata specification (https://doi-do.github.io/dcat-us/).

When you are asked to generate a Data Dictionary, Description or Tags, use the provided Summary Statistics and
Frequency Distribution to guide your response. They both describe the same Dataset and are joined on the `field` column.

The provided Summary Statistics is a CSV file. Each record contains statistics for each Dataset field.

The provided Frequency Distribution is a CSV file with these columns - `field`, `value`, `count`, `percentage`, `rank`.
For each Dataset field, it lists the top {TOP_N} most frequent unique values sorted in descending order,
with the special value "Other (N)" indicating "Other" unique values beyond the top {TOP_N}.
The "N" in "Other (N)" indicates the count of "Other" unique values. The "Other" category has a special rank of 0.

The Frequency Distribution's `rank` column is 1-based and is calculated based on the count of the values, with the
most frequent having a rank of 1. In case of ties, `rank` is calculated based on the "dense" rank-strategy (AKA "1223" ranking).

For Dataset fields with all unique values (i.e. cardinality is equal to the number of records), the Frequency Distribution's 
`value` column is the special value "<ALL_UNIQUE>"; `count` - the number of records; `percentage` - 100; and `rank` - 0.
"""

dictionary_prompt = """
Generate Labels and Descriptions for each field in the Dataset based on the Summary Statistics and Frequency Distribution provided below.

For each field, provide:
- Label: a human-friendly label for this field (e.g., "Unique Key", "Created Date", "Agency Name")
- Description: a full description for this field (can be multiple sentences)

Return the results in JSON format where each field name is a key, and the value is an object with "label" and "description" properties:
{{
  "field_name_1": {{
    "label": "Human-friendly label",
    "description": "Full description of the field"
  }},
  "field_name_2": {{
    "label": "Another label",
    "description": "Another description"
  }}
}}

Generate Labels and Descriptions for ALL fields in the Dataset. Use the Summary Statistics and Frequency Distribution to understand the context and meaning of each field.

Let's think step by step, correcting yourself as needed.

---

Summary Statistics (CSV):

{STATS}

Frequency Distribution (CSV):

{FREQUENCY}
"""

description_prompt = """
Generate a Description based on the following Summary Statistics and Frequency Distribution data about the Dataset.

Let's think step by step.

---

Summary Statistics (CSV):

{STATS}

Frequency Distribution (CSV):

{FREQUENCY}

---

Do not output the summary statistics for each field. Do not output the frequency for each field.
Do not output data about each field individually, but instead output about the dataset as a whole
in one 1-8 sentence description.

After the Description, add a section titled "Notable Characteristics" with a bulleted list of notable
characteristics of the Dataset (e.g. the central tendency and spread of the data, the distribution shape,
anomalies, patterns; if there are any outliers, missing values, duplicates, PII/PHI/PCI data; and
other data quality issues that the User should be aware of).

Add an Attribution with the placeholder "{GENERATED_BY_SIGNATURE}" at the bottom of the output.

The entire output should be in Markdown format.
"""

tags_prompt = """
A Tag is a keyword or label that categorizes datasets with other, similar datasets.
Using the right Tags makes it easier for others to find and use datasets.

{TAG_VOCAB}

Add an Attribution with the placeholder "{GENERATED_BY_SIGNATURE}" after the Tags. If generating JSON format,
add the Attribution as a separate key at the top level of the JSON object, after the Tags, otherwise add it
at the bottom of the Tags in Markdown format.

Let's think step by step.

---

Summary Statistics (CSV):

{STATS}

Frequency Distribution (CSV):

{FREQUENCY}"""

prompt = "What is this dataset about?"

custom_prompt_guidance = """
We need to answer the User's Prompt above. Let's think step by step, correcting yourself as needed.

If the User's Prompt is not about the Dataset, immediately return
'I'm sorry. I'm afraid I can only answer questions about the Dataset.'

If the User's Prompt can be answered by using the Dataset's Summary Statistics and
Frequency Distribution data below, immediately return the answer.

Otherwise, using the Dataset's Summary Statistics, Frequency Distribution and Data Dictionary below,
create a SQL query that can be used to answer the User's Prompt.

SQL Query Generation Guidelines:

END SQL Query Generation Guidelines
- Make sure the generated SQL query is valid and has comments to explain the query
- Add "-- {GENERATED_BY_SIGNATURE}" at the top of the query

Return the SQL query as a SQL code block preceded by a newline.

---

Summary Statistics (CSV):

{STATS}

Frequency Distribution (CSV):

{FREQUENCY}

Data Dictionary:

{DICTIONARY}"""

### DuckDB SQL "One-Shot" Query Generation Guidelines
### This list replaces the "END SQL Query Generation Guidelines" marker in custom_prompt_guidance
duckdb_sql_guidance = """
- Use DuckDB {DUCKDB_VERSION} syntax
- The input csv has headers and uses {DELIMITER} as the delimiter
- Column names with spaces and special characters are case-sensitive and should be enclosed in double quotes
- Only use the `read_csv_auto` table function to read the input CSV
- Use the placeholder {INPUT_TABLE_NAME} for the input csv in the `read_csv_auto` table function call
- Remember that the date format specifier is the second parameter for the date functions `strftime()` and `strptime()`
- Remember that the date format specifier for seconds is `%S`, NOT `%s`
"""

### Polars SQL "One-Shot" Query Generation Guidelines
### This list replaces the "END SQL Query Generation Guidelines" marker in custom_prompt_guidance
polars_sql_guidance = """
- Use Polars SQL syntax (which is a dialect of PostgreSQL)
- Use the Dataset's Summary Statistics, Frequency Distribution and Data Dictionary data to generate the SQL query
- Use {INPUT_TABLE_NAME} as the placeholder for the table name to query
- Column names with embedded spaces and special characters are case-sensitive and should be enclosed in double quotes
- Only use SQL functions that are supported by Polars SQL.
  Refer to https://github.com/pola-rs/polars/blob/e2818b3db9be5ec6b9abcc873bc4d2ab92861861/crates/polars-sql/src/functions.rs#L37-L755
  Note that we have the "rank" and "list_eval" polars features enabled.
- `datepart`'s syntax is `date_part('part', date_column)` where part is one of: "year", "month", "week", "day", "hour", "minute", "second",
  "millisecond", "microsecond", "nanosecond", "epoch", "doy", "dow", "week", "timezone", "time"
- Always cast columns to date/datetime type before doing date operations
- Do not use IN (subquery), SIMILAR TO and CROSS JOIN in the generated SQL query
- Only use comments with the `--` prefix. Comments can only start on a new line. No inline comments
- Do not use comments using the `/*` and `*/` syntax, as Polars SQL does not support it
"""

### Additional DuckDB SQL Few-Shot Learning Examples when --fewshot-examples is set
dd_fewshot_examples = """
## Few-Shot Learning Examples

Here are examples of common query patterns to follow:

### Example 1: Basic Aggregation
**User Question:** "What is the kurtosis of a given numerical column?"
**SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Calculate kurtosis of a given numerical column
SELECT kurtosis(column_name) as kurtosis_value
FROM read_csv_auto({INPUT_TABLE_NAME});
```

### Example 2: Group By with Aggregation
**User Question:** "Show me the count by category"
**SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Group records by category and count occurrences
SELECT 
    category,
    COUNT(*) as count
FROM read_csv_auto({INPUT_TABLE_NAME})
GROUP BY category
ORDER BY count DESC;
```

### Example 3: Date/Time Analysis
**User Question:** "What are the trends over time?"
**SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Analyze trends by date, assuming there's a date column
SELECT 
    DATE(date_column) as date,
    COUNT(*) as daily_count,
    AVG(numeric_column) as avg_value
FROM read_csv_auto({INPUT_TABLE_NAME})
WHERE date_column IS NOT NULL
GROUP BY DATE(date_column)
ORDER BY date;
```

### Example 4: Top N Analysis
**User Question:** "What are the top 10 most frequent values?"
**SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Find top 10 most frequent values in a column
SELECT 
    column_name,
    COUNT(*) as frequency
FROM read_csv_auto({INPUT_TABLE_NAME})
WHERE column_name IS NOT NULL
GROUP BY column_name
ORDER BY frequency DESC
LIMIT 10;
```

### Example 5: Conditional Analysis
**User Question:** "How many records meet specific criteria?"
**SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Count records that meet specific conditions
SELECT 
    COUNT(*) as matching_records,
    COUNT(*) * 100.0 / (SELECT COUNT(*) FROM read_csv_auto({INPUT_TABLE_NAME})) as percentage
FROM read_csv_auto({INPUT_TABLE_NAME})
WHERE condition1 = 'value1' 
  AND condition2 > 100;
```

**User Question:** "What's the relationship between two columns?"
**SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Calculate relationship between two columns
SELECT corr(column2, column1) as correlation,
       covar_pop(column2, column1) as covariance_population,
       covar_samp(column2, column1) as covariance_sample,
       regr_slope(column2, column1) as regression_slope,
       regr_intercept(column2, column1) as regression_intercept,
       regr_r2(column2, column1) as regression_r_squared,
       regr_count(column2, column1) as regression_count
FROM read_csv_auto({INPUT_TABLE_NAME});
```

### Example 7: Complex Filtering with Subqueries
**User Question:** "Show records above the average"
**SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Find records above the average value
SELECT *
FROM read_csv_auto({INPUT_TABLE_NAME})
WHERE numeric_column > (
    SELECT AVG(numeric_column) 
    FROM read_csv_auto({INPUT_TABLE_NAME})
    WHERE numeric_column IS NOT NULL
)
ORDER BY numeric_column DESC;
```

### Example 8: Window Functions for Ranking
**User Question:** "Rank the data by some criteria"
**SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Rank records using window functions
SELECT 
    *,
    ROW_NUMBER() OVER (ORDER BY numeric_column DESC) as rank,
    RANK() OVER (ORDER BY numeric_column DESC) as rank_with_ties
FROM read_csv_auto({INPUT_TABLE_NAME})
ORDER BY rank;
```

### Best Practices:
- Always use `read_csv_auto({INPUT_TABLE_NAME})` to read the CSV
- Include meaningful column aliases (e.g., `COUNT(*) as total_count`)
- Add WHERE clauses to filter out NULL values when appropriate
- Use ORDER BY for meaningful result ordering
- Include LIMIT for large result sets
- Add comments explaining the query logic
- Use appropriate aggregation functions (COUNT, SUM, AVG, MIN, MAX)
- Consider using CTEs (WITH clauses) for complex queries
- Use CASE statements for conditional logic
- Leverage window functions for ranking and running totals"""

### Additional Polars SQL Few-Shot Learning Examples when --fewshot-examples is set
p_fewshot_examples = """
## Few-Shot Learning Examples

Polars SQL is a SQL dialect patterned after PostgreSQL's syntax,
but with Polars-specific functions and operators.

Here are examples of common Polars SQL query patterns to follow:

### Example 1: Basic Aggregation
**User Question:** "What is the continuous quantile element of a given numerical column?"
**Polars SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Calculate kurtosis of a given numerical column
SELECT QUANTILE_CONT(column_name, 0.30) as column_name_q30
FROM {INPUT_TABLE_NAME};
```

### Example 2: Group By with Aggregation
**User Question:** "Show me the count by category"
**Polars SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Group records by category and count occurrences
SELECT 
    category,
    COUNT(*) as count
FROM {INPUT_TABLE_NAME}
GROUP BY category
ORDER BY count DESC;
```

### Example 3: Date/Time Analysis
**User Question:** "What are the trends over time?"
**Polars SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Analyze trends by date, assuming there's a date column
SELECT 
    date_column,
    COUNT(*) as daily_count,
    AVG(numeric_column) as avg_value
FROM {INPUT_TABLE_NAME}
WHERE date_column IS NOT NULL
GROUP BY date_part('day', date_column)
ORDER BY date_column;
```

### Example 4: Top N Analysis
**User Question:** "What are the top 10 most frequent values?"
**Polars SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE} 
-- Find top 10 most frequent values in a column
SELECT 
    column_name,
    COUNT(*) as frequency
FROM {INPUT_TABLE_NAME}
WHERE column_name IS NOT NULL
GROUP BY column_name
ORDER BY frequency DESC
LIMIT 10;
```

### Example 5: Conditional Analysis
**User Question:** "How many records meet specific criteria?"
**Polars SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Count records that meet specific conditions
SELECT 
    COUNT(*) as matching_records,
    COUNT(*) * 100.0 / (SELECT COUNT(*) FROM {INPUT_TABLE_NAME}) as percentage
FROM {INPUT_TABLE_NAME}
WHERE condition1 = 'value1' 
  AND condition2 > 100;
```

### Example 6: Cross-Column Analysis
**User Question:** "What's the relationship between two columns?"
**Polars SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Calculate relationship between two columns
SELECT 
    corr(column1, column2) as pearson_correlation,
    covar(column1, column2) as covariance
FROM {INPUT_TABLE_NAME};
```

### Example 7: Complex Filtering with Subqueries
**User Question:** "Show records above the average"
**Polars SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Find records above the average value
SELECT *
FROM {INPUT_TABLE_NAME}
WHERE numeric_column > (
    SELECT AVG(numeric_column) 
    FROM {INPUT_TABLE_NAME}
    WHERE numeric_column IS NOT NULL
)
ORDER BY numeric_column DESC;
```

### Example 8: Window Functions for Ranking
**User Question:** "Rank the data by some criteria"
**Polars SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Rank records using window functions
SELECT 
    *,
    ROW_NUMBER() OVER (ORDER BY numeric_column DESC) as rank,
    RANK() OVER (ORDER BY numeric_column DESC) as rank_with_ties
FROM {INPUT_TABLE_NAME}
ORDER BY rank;
```

### Example 9: String Operations
**User Question:** "Extract patterns from text columns"
**Polars SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Extract patterns and analyze string data
SELECT 
    text_column,
    LENGTH(text_column) as text_length,
    UPPER(text_column) as uppercase_text,
    LOWER(text_column) as lowercase_text,
    SUBSTR(text_column, 1, 10) as first_10_chars,
    TIMESTAMP(text_column, 'yyyy-MM-dd HH:mm:ss') as timestamp_column,
    DATE(text_column, 'yyyy-MM-dd') as date_column,
    REGEXP_LIKE(text_column, 'regex_pattern') as is_regex_pattern_present,
    STRPOS(text_column, 'substring') as substring_position,
    STARTS_WITH(text_column, 'prefix') as starts_with_prefix,
    ENDS_WITH(text_column, 'suffix') as ends_with_suffix,
    REPLACE(text_column, 'old_substring', 'new_substring') as replaced_text,
    CONCAT(text_column, 'concatenated_text') as concatenated_text,
    TRIM(text_column) as trimmed_text,
    LTRIM(text_column) as ltrimmed_text,
    RTRIM(text_column) as rtrimmed_text
FROM {INPUT_TABLE_NAME}
WHERE text_column IS NOT NULL;
```

### Example 10: Array Operations
**User Question:** "Work with array columns"
**Polars SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Analyze array column data
SELECT 
    array_column,
    ARRAY_AGG(array_column ORDER BY array_column) as array_agg_sorted,
    ARRAY_CONTAINS(array_column, 'value') as contains_value,
    ARRAY_GET(array_column, 1) as array_col_at_1,
    ARRAY_LENGTH(array_column) as array_length,
    ARRAY_MAX(array_column) as max_value,
    ARRAY_MIN(array_column) as min_value
FROM {INPUT_TABLE_NAME}
WHERE array_column IS NOT NULL;
```

### Example 11: Struct Operations
**User Question:** "Access nested struct fields"
**Polars SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Extract and analyze nested struct data
SELECT 
    struct_column.field1 as extracted_field1,
    struct_column.field2 as extracted_field2,
    struct_column.nested.field3 as nested_field3
FROM {INPUT_TABLE_NAME}
WHERE struct_column IS NOT NULL;
```

### Example 12: Pivot Operations
**User Question:** "Create a pivot table"
**Polars SQL Query:**
```sql
-- {GENERATED_BY_SIGNATURE}
-- Create a pivot table from the data
SELECT 
    category,
    SUM(CASE WHEN status = 'active' THEN value ELSE 0 END) as active_sum,
    SUM(CASE WHEN status = 'inactive' THEN value ELSE 0 END) as inactive_sum,
    COUNT(CASE WHEN status = 'active' THEN 1 END) as active_count
FROM {INPUT_TABLE_NAME}
GROUP BY category;
```

### Best Practices for Polars SQL:
- Always use `{INPUT_TABLE_NAME}` as the placeholder for the table name
- Include meaningful column aliases (e.g., `COUNT(*) as total_count`)
- Add WHERE clauses to filter out NULL values when appropriate
- Use ORDER BY for meaningful result ordering
- Include LIMIT for large result sets
- Add comments explaining the query logic
- Use appropriate aggregation functions (COUNT, SUM, AVG, MIN, MAX, kurtosis, skewness)
- Leverage Polars-specific functions like `date_part` and `array_*`
- Use CASE statements for conditional logic and pivoting
- Leverage window functions for ranking and running totals
- Take advantage of Polars' optimized array operations
- Use `date_part` for time-based grouping instead of `DATE()` function
- Consider using CTEs (WITH clauses) for complex queries
- Leverage Polars' built-in statistical functions for data analysis"""
